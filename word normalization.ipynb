{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#the text for analysis\n",
    "text_en = \"Artificial Intelligence (AI) is likely to be either the best or the worst thing to happen to humanity\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=7c282ee7e052fec510eaf32d403bcf444997e2b67711dde46113e4aa58945ca2\n",
      "  Stored in directory: c:\\users\\aswin\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#install langdetect libarary\n",
    "!pip install langdetect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import library\n",
    "import langdetect\n",
    "from langdetect import detect_langs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[en:0.9999959616789433]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(detect_langs(text_en))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hi = \"भेजना चाहते हैं हिंदी में मैसेज लेकिन नहीं आती टाइपिंग? इन आसान Tips से मोबाइल से भेजें हिंदी में टेक्स्ट मैसेज\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hi:0.9999984294755601]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(detect_langs(text_hi))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_ml = \"ലോകത്തെ വായുമലിനീകരണം കൂടിയ 50 നഗരങ്ങളില്‍ 39 എണ്ണവും ഇന്ത്യയില്‍\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ml:0.9999993528475908]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(detect_langs(text_ml))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print number of characters\n",
    "len(text_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print number of distinct characters\n",
    "len(set(text_en))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '(', ')', 'A', 'I', 'a', 'b', 'c', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#printing unique characters\n",
    "print(sorted(set(text_en)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB 660.6 kB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.0/1.5 MB 393.8 kB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.0/1.5 MB 393.8 kB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.0/1.5 MB 393.8 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/1.5 MB 504.4 kB/s eta 0:00:03\n",
      "     ------- -------------------------------- 0.3/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.5/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.5/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.5/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.8/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.3/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.4/1.5 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.4/1.5 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/de/cd/d80c9e284ae6c1b2172dacf0651d25b78ee1f7efbc12d74ea7b87c766263/regex-2023.8.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.3/268.3 kB 17.2 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'likely', 'to', 'be', 'either', 'the', 'best', 'or', 'the', 'worst', 'thing', 'to', 'happen', 'to', 'humanity']\n"
     ]
    }
   ],
   "source": [
    "#text tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text_en)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#printing the type of the variable\n",
    "type(tokenized_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', '(', 'ai', ')', 'is', 'likely', 'to', 'be', 'either', 'the', 'best', 'or', 'the', 'worst', 'thing', 'to', 'happen', 'to', 'humanity']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tk_low = [w.lower() for w in tokenized_word]\n",
    "print(tk_low)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Using cached anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.0.0-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: It would be 100% unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\n",
      "\n",
      "\n",
      "\n",
      "Expanded_text: It would be 100% unfair to demand that people cease pirating files when those same people are not paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I would like to see everyone eventually pay for music and the like, I would not ask for it until there is reciprocity.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "# contracted text\n",
    "text = '''It would be 100% unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.'''\n",
    " \n",
    "# creating an empty list\n",
    "expanded_words = []   \n",
    "for word in text.split():\n",
    "  # using contractions.fix to expand the shortened words\n",
    "  expanded_words.append(contractions.fix(word))  \n",
    "   \n",
    "expanded_text = ' '.join(expanded_words)\n",
    "print('Original text: ' + text)\n",
    "print(\"\\n\\n\")\n",
    "print('Expanded_text: ' + expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'would', 'be', '100', '%', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', \"n't\", 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', '.', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', ',', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', '.', 'While', 'I', \"'d\", 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciprocity', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sent = word_tokenize(text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', 'While', 'I', 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', 'I', 'not', 'ask', 'for', 'it', 'until', 'there', 'reciprocity']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "def remove_punct(token):\n",
    " return [word for word in token if word.isalpha()]\n",
    "sent = remove_punct(sent)\n",
    "print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'peopl', 'ceas', 'pirat', 'file', 'when', 'those', 'same', 'peopl', 'are', 'paid', 'for', 'their', 'particip', 'in', 'veri', 'lucr', 'network', 'scheme', 'ordinari', 'peopl', 'are', 'relentlessli', 'spi', 'on', 'and', 'not', 'compens', 'for', 'inform', 'taken', 'from', 'them', 'while', 'i', 'like', 'to', 'see', 'everyon', 'eventu', 'pay', 'for', 'music', 'and', 'the', 'like', 'i', 'not', 'ask', 'for', 'it', 'until', 'there', 'reciproc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps_stem_sent = [ps.stem(words_sent) for words_sent in sent]\n",
    "print(ps_stem_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'file', 'when', 'those', 'same', 'people', 'are', 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'scheme', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', 'While', 'I', 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', 'I', 'not', 'ask', 'for', 'it', 'until', 'there', 'reciprocity']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_sent = [lemmatizer.lemmatize(words_sent) for words_sent in sent]\n",
    "print(lem_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kerala', 'NOUN'), ('University', 'NOUN'), ('of', 'ADP'), ('Digital', 'NOUN'), ('Sciences', 'NOUN'), ('Innovation', 'NOUN'), ('and', 'CONJ'), ('Technology', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('Public', 'ADJ'), ('State', 'NOUN'), ('University', 'NOUN'), ('established', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('year', 'NOUN'), ('2020', 'NUM'), ('.', '.'), ('DUK', 'NOUN'), ('is', 'VERB'), ('located', 'VERB'), ('in', 'ADP'), ('Thiruvananthapuram', 'NOUN'), (',', '.'), ('Kerala', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Kerala University of Digital Sciences Innovation and Technology is a Public State University established in the year 2020. DUK is located in Thiruvananthapuram, Kerala\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "tags = nltk.pos_tag(tokens, tagset = \"universal\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance\n",
      "  Downloading editdistance-0.6.2-cp311-cp311-win_amd64.whl (22 kB)\n",
      "Installing collected packages: editdistance\n",
      "Successfully installed editdistance-0.6.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#EDIT DISTANCE\n",
    "!pip install editdistance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import editdistance\n",
    "editdistance.eval('banana', 'bahama')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editdistance.eval('hello', 'hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "editdistance.eval('kel', 'hello')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>=3.1->textblob) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#spelling correction\n",
    "!pip install textblob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a good university and always value their employees.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import TextBlob\n",
    "from textblob import TextBlob\n",
    "sen = TextBlob(\"it is a good univesity and alays value ttheir employees.\")\n",
    "print(sen.correct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
